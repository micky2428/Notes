{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Course 2025 HW2\n",
        "The code scripts are from [aideml](https://github.com/WecoAI/aideml) project on github with some modifications.\n",
        "\n",
        "AIDE: AI-Driven Exploration in the Space of Code\n",
        "\n",
        "這些程式碼腳本來自 GitHub 上的 AIDEML 項目，並做了一些修改。\n",
        "\n",
        "https://arxiv.org/pdf/2502.13138\n",
        "\n",
        "#strong baseline 0.84773\n",
        "#medium baseline 0.91179\n",
        "#sinple baseline 1.31311\n",
        "\n",
        "#同學最高0.51943\n",
        "#因為競賽使用的是誤差類指標（如 RMSE），0.5 比 1 更好，因為誤差更小。\n",
        "\n",
        "<font color='red' size=6>Make a copy before running or editing the code.</font>\n",
        "\n",
        "論文\n",
        "研究重點\n",
        "這篇論文的研究重點是介紹並評估一個名為 AIDE（AI-Driven Exploration） 的基於大型語言模型（LLM）的代理（Agent），其目標是自動化機器學習工程中的試錯過程，並在程式碼解決方案空間中進行系統性探索。AIDE 旨在解決傳統機器學習工程中需要大量人工介入的問題，通過結合樹搜索方法與 LLM 的能力，實現高效的程式碼生成、調試和優化，從而在多種機器學習任務中達到競爭性甚至超越人類的表現。\n",
        "\n",
        "研究原因\n",
        "現有方法的局限性：\n",
        "傳統的通用 LLM 代理（如 ReACT）將歷史觀察附加到上下文，依賴模型一次性解決優化問題，但隨著歷史數據增加，提示詞（prompt）會變得過大，限制了可擴展性。\n",
        "傳統 AutoML 方法（如 Auto-WEKA、TPOT）依賴靜態搜索空間，缺乏動態適應複雜問題的能力。\n",
        "這些方法無法有效分解結構化問題，或在需要大量迭代的任務中保持高效。\n",
        "LLM 的潛力：\n",
        "大型語言模型具備廣泛的領域知識，能夠生成和改進程式碼解決方案，但需要一個結構化的框架來引導其探索過程。\n",
        "現有方法未充分利用 LLM 在程式碼空間中的迭代優化潛力。\n",
        "實際需求：\n",
        "機器學習工程中的試錯過程耗時且依賴專業知識，自動化此過程可以顯著提高效率並降低成本，尤其是在 Kaggle 競賽或 AI 研發等場景中。\n",
        "研究方法\n",
        "AIDE 的設計：\n",
        "框架：AIDE 將任務建模為優化問題，使用樹結構組織歷史解決方案，並通過 LLM 基於單個樹節點提出改進建議。\n",
        "核心組件：\n",
        "搜索策略（π）：選擇下一個待改進的解決方案。\n",
        "編碼操作（f）：生成新程式碼，執行單一「原子」改進（如更改優化器或添加正則化）。\n",
        "總結操作（Σ(T)）：從樹中提取關鍵信息（如性能指標、超參數設置），保持提示詞簡潔。\n",
        "數據預覽：在提示中提供數據集的基本元數據（如行數、列名），幫助 LLM 做出關鍵決策。\n",
        "算法：AIDE 使用硬編碼的樹搜索算法，根據自動評估逐步累積改進（見 Algorithm 1）。\n",
        "評估方法：\n",
        "Weco-Kaggle 基準測試：\n",
        "在 63 個 Kaggle 競賽（包括 Weco-Kaggle Lite 的 16 個表格任務）上測試 AIDE，與人類參與者和基線（如 H2O AutoML、AutoGPT、人類+ChatGPT）比較。\n",
        "指標包括「超越人類百分比（Exceeds % of Humans）」和「高於中位數（Above Median）」。\n",
        "MLE-Bench：\n",
        "在 OpenAI 的 75 個 Kaggle 競賽基準中評估 AIDE，與其他代理（如 OpenHands）比較，重點關注提交有效率和獎牌獲得率。\n",
        "RE-Bench：\n",
        "在 METR 的 AI 研發任務中測試 AIDE（如 Triton Kernel 優化），與頂尖人類專家比較，分析其在時間限制下的表現。\n",
        "實驗細節：\n",
        "使用多種 LLM（如 GPT-4 Turbo、o1-preview、Llama 3.1、Claude 3.5）測試 AIDE 的性能。\n",
        "通過多次運行（不同種子）計算平均值和標準誤差，確保結果穩健性。\n",
        "結論\n",
        "性能表現：\n",
        "Weco-Kaggle：AIDE（使用 GPT-4 Turbo）平均超越 51.38% 的人類參與者，在 50% 的競賽中高於中位數，優於 H2O AutoML（35.34%）和 AutoGPT（32.34%）。\n",
        "MLE-Bench：AIDE 搭配 o1-preview 在 16.9% 的競賽中獲得獎牌，遠超其他代理（如\n",
        "OpenHands 的 4.4%），顯示其在迭代優化中的優勢。\n",
        "RE-Bench：AIDE 在 6 小時內超越人類專家（如在 Triton Kernel 優化中），但在需要處理大型代碼庫或多步改進的任務中表現受限。\n",
        "優勢：\n",
        "AIDE 的樹搜索方法避免了提示詞過載，保持了結構化探索，並通過系統性迭代提高了解決方案質量。\n",
        "其設計適用於多種場景，從表格機器學習到深度學習和 AI 研發任務，展現了廣泛的通用性。\n",
        "局限性與未來方向：\n",
        "AIDE 的貪婪策略可能陷入局部最優，特別是在複雜研發任務中。\n",
        "測試集與 Kaggle 私有測試集的差異可能影響評估準確性，且存在數據污染風險。\n",
        "未來可改進搜索策略（如引入非貪婪方法）並擴展到更大規模的代碼庫處理。\n",
        "總體意義：\n",
        "AIDE 提供了一個結合 LLM 提示與樹搜索的原則性方法，為自動化機器學習工程開闢了新方向，具有降低成本和提高效率的潛力。\n",
        "總結\n",
        "這篇論文通過提出 AIDE，解決了機器學習工程中自動化探索的挑戰，並通過多個基準測試證明了其有效性。研究動機源於現有方法的不足，方法上創新性地結合了樹搜索與 LLM，結論顯示 AIDE 在多數場景中表現出色，但仍有改進空間。這項工作為未來的自動化 ML 工程奠定了基礎。"
      ],
      "metadata": {
        "id": "WMksFjyDhCyG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites"
      ],
      "metadata": {
        "id": "ubu98XRV3kCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check GPU\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qY6jmqu4Nhy0",
        "outputId": "5abccb3e-2e49-452a-b10a-ee9ce9ab39ba"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Apr  3 09:21:11 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eZmTzUexsG7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac8cded8-1b21-4627-da2c-f41f844cac85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dataclasses_json==0.6.4\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting shutup==0.2.0\n",
            "  Downloading shutup-0.2.0-py3-none-any.whl.metadata (530 bytes)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses_json==0.6.4)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses_json==0.6.4)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses_json==0.6.4) (24.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses_json==0.6.4)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses_json==0.6.4) (4.13.0)\n",
            "Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Downloading shutup-0.2.0-py3-none-any.whl (1.5 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: shutup, mypy-extensions, marshmallow, typing-inspect, dataclasses_json\n",
            "Successfully installed dataclasses_json-0.6.4 marshmallow-3.26.1 mypy-extensions-1.0.0 shutup-0.2.0 typing-inspect-0.9.0\n",
            "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
            "Collecting llama-cpp-python==0.3.4\n",
            "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu122/llama_cpp_python-0.3.4-cp311-cp311-linux_x86_64.whl (445.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m445.2/445.2 MB\u001b[0m \u001b[31m139.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (4.13.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.3.4)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.3.4) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.3.4) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.4\n"
          ]
        }
      ],
      "source": [
        "# install packages\n",
        "!pip install dataclasses_json==0.6.4 shutup==0.2.0\n",
        "\n",
        "!pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset\n",
        "\n",
        "!gdown --id 1Ah5uV6cu3Bnz6WfkUuxEZCLqj5k1lbpd\n",
        "\n",
        "# Choose a workable link\n",
        "# !gdown --id 1XtF9-hGw2tKe4WvUMW5YE6lj6p1QcWIc\n",
        "# !gdown --id 1diswE_9XoT-uII23ucRppau1ErEQkY2y\n",
        "# !gdown --id 1BAVMzLZqEgtG8rwog7ttC7xKPw5QTngn\n",
        "# !gdown --id 1PAI4_3kRWwIPQMscMdGt9HLqZZy1vWSD\n",
        "\n",
        "!unzip /content/ML2025Spring-hw2-public.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXR6hQIa5sML",
        "outputId": "6096a473-3bde-4e3b-f3b1-f9f83c01b85e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Ah5uV6cu3Bnz6WfkUuxEZCLqj5k1lbpd\n",
            "To: /content/ML2025Spring-hw2-public.zip\n",
            "100% 621k/621k [00:00<00:00, 95.2MB/s]\n",
            "Archive:  /content/ML2025Spring-hw2-public.zip\n",
            "   creating: ML2025Spring-hw2-public/\n",
            "  inflating: ML2025Spring-hw2-public/sample_submission.csv  \n",
            "  inflating: ML2025Spring-hw2-public/test.csv  \n",
            "  inflating: ML2025Spring-hw2-public/train.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hw(3/4):嘗試換模型(優先選~10GBs模型，太大就要看自己的硬體了)\n",
        "# ========================== TODO: try different LLM ==========================\n",
        "# Hugging Face: https://huggingface.co/models?library=gguf\n",
        "# OpenLLM Leaderboard: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?params=7%2C65&official=true\n",
        "# remember to replace 'blob' with 'resolve' in the link you copy.\n",
        "!wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3uy2PgYe7_T",
        "outputId": "1021958d-c525-45cc-fca8-75a120ac12d8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-03 09:21:52--  https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.163.189.114, 3.163.189.74, 3.163.189.90, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.163.189.114|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/d6/e9/d6e9318f285870e2a0e3056e22f9c7ec90cd13e14cfde122129ae66af9ad788f/9da71c45c90a821809821244d4971e5e5dfad7eb091f0b8ff0546392393b6283?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%3B+filename%3D%22Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%22%3B&Expires=1743675713&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MzY3NTcxM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2Q2L2U5L2Q2ZTkzMThmMjg1ODcwZTJhMGUzMDU2ZTIyZjljN2VjOTBjZDEzZTE0Y2ZkZTEyMjEyOWFlNjZhZjlhZDc4OGYvOWRhNzFjNDVjOTBhODIxODA5ODIxMjQ0ZDQ5NzFlNWU1ZGZhZDdlYjA5MWYwYjhmZjA1NDYzOTIzOTNiNjI4Mz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=buIawITkWKAhXO7vxI3De2W99-7HYkKqLHST98okH7VnlDEYbA%7Ejrb8TNmgiGQUTDvrRapEOybVhopcaTfjZwY6P3Vd42%7E8G4r%7E%7Eld0EHFEu9Anj9xKA9hGkk9Oqs4Er9CqQij3hX7NjPhvhLSMqZp%7E7I0s1Thh%7EpGdtOYHZCQomblWbA3W4LbuwY6HuJb9LswfJ5rS5sabeYKD-f2H89DflpaQmOabEY72dYgY9V2G8qN3%7EEW3rwFXg56lu8AQGDw21UZrTyAlrKwG%7EF994vbzsUw34XHgTASe3TtYXzklqLErOrEGYFFj4evtBFnhI7cETGYwA3tRR5INRmYQVkw__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-04-03 09:21:53--  https://cdn-lfs-us-1.hf.co/repos/d6/e9/d6e9318f285870e2a0e3056e22f9c7ec90cd13e14cfde122129ae66af9ad788f/9da71c45c90a821809821244d4971e5e5dfad7eb091f0b8ff0546392393b6283?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%3B+filename%3D%22Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%22%3B&Expires=1743675713&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MzY3NTcxM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2Q2L2U5L2Q2ZTkzMThmMjg1ODcwZTJhMGUzMDU2ZTIyZjljN2VjOTBjZDEzZTE0Y2ZkZTEyMjEyOWFlNjZhZjlhZDc4OGYvOWRhNzFjNDVjOTBhODIxODA5ODIxMjQ0ZDQ5NzFlNWU1ZGZhZDdlYjA5MWYwYjhmZjA1NDYzOTIzOTNiNjI4Mz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=buIawITkWKAhXO7vxI3De2W99-7HYkKqLHST98okH7VnlDEYbA%7Ejrb8TNmgiGQUTDvrRapEOybVhopcaTfjZwY6P3Vd42%7E8G4r%7E%7Eld0EHFEu9Anj9xKA9hGkk9Oqs4Er9CqQij3hX7NjPhvhLSMqZp%7E7I0s1Thh%7EpGdtOYHZCQomblWbA3W4LbuwY6HuJb9LswfJ5rS5sabeYKD-f2H89DflpaQmOabEY72dYgY9V2G8qN3%7EEW3rwFXg56lu8AQGDw21UZrTyAlrKwG%7EF994vbzsUw34XHgTASe3TtYXzklqLErOrEGYFFj4evtBFnhI7cETGYwA3tRR5INRmYQVkw__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 18.65.229.34, 18.65.229.76, 18.65.229.6, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|18.65.229.34|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8540775840 (8.0G) [binary/octet-stream]\n",
            "Saving to: ‘Meta-Llama-3.1-8B-Instruct-Q8_0.gguf’\n",
            "\n",
            "Meta-Llama-3.1-8B-I 100%[===================>]   7.95G  91.4MB/s    in 50s     \n",
            "\n",
            "2025-04-03 09:22:43 (163 MB/s) - ‘Meta-Llama-3.1-8B-Instruct-Q8_0.gguf’ saved [8540775840/8540775840]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# Load the model onto GPU\n",
        "myModel = Llama(\n",
        "    # ========================== TODO: try different LLM ==========================\n",
        "    # Before changing LLM, restart the session!\n",
        "    \"/content/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\",\n",
        "    verbose=False,\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=8192,    # This argument is how many tokens the model can take. The longer the better, but it will consume more memory.\n",
        ")\n",
        "# HW(4/4)使用時要調整temperature\n",
        "def generate_response(_model: Llama, _messages: str) -> str:\n",
        "    '''\n",
        "    This function will inference the model with given messages.\n",
        "    '''\n",
        "    _output = _model.create_chat_completion(\n",
        "        _messages,\n",
        "        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],\n",
        "        max_tokens=4096,    # This argument is how many tokens the model can generate.\n",
        "        temperature=0.5,      # This argument is the randomness of the model. 0 means no randomness. We suggest setting the temperature value to 0 for reproducibility.\n",
        "    )[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return _output"
      ],
      "metadata": {
        "id": "ZozqjSIgfAuJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e25fc453-ac9a-402e-93c7-2b6121374129"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_new_context_with_model: n_ctx_per_seq (8192) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "MgRw-Rt740fF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ],
      "metadata": {
        "id": "Jf1drXU_MvAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# best solution 和 good solutions 會產生feedback讓你改善程式碼(可能會跳錯)\n",
        "# Define a function to save the best solution and other good solutions to files.\n",
        "def save_run(cfg, journal):\n",
        "    # Retrieve and save the best found solution.\n",
        "    best_node = journal.get_best_node(only_good=False)  # Get the best node.\n",
        "    with open(\"best_solution.py\", \"w\") as f:\n",
        "        f.write(best_node.code)\n",
        "\n",
        "    good_nodes = journal.get_good_nodes()  # Retrieve all good solution nodes.\n",
        "    for i, node in enumerate(good_nodes):\n",
        "        filename = f\"good_solution_{i}.py\"\n",
        "        with open(filename, \"w\") as f:\n",
        "            f.write(node.code)"
      ],
      "metadata": {
        "id": "cBIdD6RrMuY5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpreter (DO NOT MODIFY THIS CELL)"
      ],
      "metadata": {
        "id": "XJIf1li3ifQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "DO NOT MODIFY THIS CELL\n",
        "\n",
        "Python interpreter for executing code snippets and capturing their output.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import queue\n",
        "import signal\n",
        "import sys\n",
        "import time\n",
        "import traceback\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from shutil import rmtree\n",
        "import shutil\n",
        "from multiprocessing import Process, Queue\n",
        "from typing import Hashable, cast\n",
        "\n",
        "import humanize\n",
        "import rich\n",
        "import shutup\n",
        "from rich.logging import RichHandler\n",
        "from rich.syntax import Syntax\n",
        "from dataclasses import dataclass\n",
        "from dataclasses_json import DataClassJsonMixin\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ExecutionResult(DataClassJsonMixin):\n",
        "    \"\"\"\n",
        "    Result of executing a code snippet in the interpreter.\n",
        "    Contains the output, execution time, and exception information.\n",
        "    儲存執行後的結果(如執行時間,錯誤等資訊)\n",
        "    \"\"\"\n",
        "    term_out: list[str]\n",
        "    exec_time: float\n",
        "    exc_type: str | None\n",
        "    exc_info: dict | None = None\n",
        "    exc_stack: list[tuple] | None = None\n",
        "\n",
        "def exception_summary(e, exec_file_name):\n",
        "    \"\"\"Generates a string that summarizes an exception and its stack trace\"\"\"\n",
        "    tb_lines = traceback.format_exception(e)\n",
        "    # Combine the traceback lines into a single string, skipping lines that contain \"importlib\".\n",
        "    tb_str = \"\".join(\n",
        "        [\n",
        "            line\n",
        "            for line in tb_lines\n",
        "            # if \"importlib\" not in line  # Filter out unwanted traceback lines.\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    exc_info = {}\n",
        "    if hasattr(e, \"args\"):\n",
        "        exc_info[\"args\"] = [str(i) for i in e.args]  # Store the exception arguments as strings.\n",
        "    for att in [\"name\", \"msg\", \"obj\"]:\n",
        "        if hasattr(e, att):\n",
        "            exc_info[att] = str(getattr(e, att))  # Store additional attributes if available.\n",
        "\n",
        "    tb = traceback.extract_tb(e.__traceback__)  # Extract the traceback information.\n",
        "    # Create a list of tuples for each frame in the traceback.\n",
        "    exc_stack = [(t.filename, t.lineno, t.name, t.line) for t in tb]\n",
        "\n",
        "    return tb_str, e.__class__.__name__, exc_info, exc_stack  # Return the formatted traceback and exception details.\n",
        "\n",
        "# Define a class that redirects write operations to a multiprocessing queue.\n",
        "# process管理\n",
        "class RedirectQueue:\n",
        "    def __init__(self, queue, timeout=5):\n",
        "        self.queue = queue  # Store the provided queue.\n",
        "        self.timeout = timeout  # Set the timeout for queue operations.\n",
        "\n",
        "    def write(self, msg):\n",
        "        try:\n",
        "            self.queue.put(msg, timeout=self.timeout)  # Attempt to put the message into the queue.\n",
        "        except queue.Full:\n",
        "            print.warning(\"Queue write timed out\")  # Warn if the queue is full and the write times out.\n",
        "\n",
        "    def flush(self):\n",
        "        pass  # No operation is needed for flushing in this context.\n",
        "\n",
        "# Define the Interpreter class that simulates a standalone Python REPL.\n",
        "class Interpreter:\n",
        "    def __init__(\n",
        "        self,\n",
        "        timeout: int = 3600,  # Default timeout of 3600 seconds. 這份作業通常要跑1個小時結果不會太好，不用跑那麼久\n",
        "        agent_file_name: str = \"runfile.py\",  # Default file name for writing the agent's code.\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Simulates a standalone Python REPL with an execution time limit.\n",
        "\n",
        "        Args:\n",
        "            timeout (int, optional): Timeout for each code execution step. Defaults to 3600.\n",
        "            agent_file_name (str, optional): The name for the agent's code file. Defaults to \"runfile.py\".\n",
        "        \"\"\"\n",
        "        self.timeout = timeout  # Save the timeout value.\n",
        "        self.agent_file_name = agent_file_name  # Save the agent file name.\n",
        "        self.process: Process = None  # Initialize the process attribute (will hold the child process).\n",
        "\n",
        "    def child_proc_setup(self, result_outq: Queue) -> None:\n",
        "        # Import shutup to suppress warnings in the child process.\n",
        "        import shutup\n",
        "\n",
        "        shutup.mute_warnings()  # Mute all warnings before further execution.\n",
        "\n",
        "        # Redirect both stdout and stderr to the provided result queue.\n",
        "        # trunk-ignore(mypy/assignment)\n",
        "        sys.stdout = sys.stderr = RedirectQueue(result_outq)\n",
        "\n",
        "    def _run_session(\n",
        "        self, code_inq: Queue, result_outq: Queue, event_outq: Queue\n",
        "    ) -> None:\n",
        "        self.child_proc_setup(result_outq)  # Set up the child process for capturing output.\n",
        "\n",
        "        global_scope: dict = {}  # Create an empty dictionary to serve as the global scope.\n",
        "        while True:  # Continuously wait for new code to execute.\n",
        "            code = code_inq.get()  # Retrieve code from the code input queue.\n",
        "            with open(self.agent_file_name, \"w\") as f:  # Open the agent file for writing.\n",
        "                f.write(code)  # Write the received code into the file.\n",
        "\n",
        "            event_outq.put((\"state:ready\",))  # Signal that the interpreter is ready to execute the code.\n",
        "            try:\n",
        "                # Compile and execute the code within the global scope.\n",
        "                exec(compile(code, self.agent_file_name, \"exec\"), global_scope)\n",
        "            except BaseException as e:\n",
        "                # If an exception occurs, generate a summary of the exception.\n",
        "                tb_str, e_cls_name, exc_info, exc_stack = exception_summary(\n",
        "                    e,\n",
        "                    self.agent_file_name,\n",
        "                )\n",
        "                result_outq.put(tb_str)  # Put the traceback string into the result queue.\n",
        "                if e_cls_name == \"KeyboardInterrupt\":\n",
        "                    e_cls_name = \"TimeoutError\"  # Convert a KeyboardInterrupt into a TimeoutError.\n",
        "\n",
        "                event_outq.put((\"state:finished\", e_cls_name, exc_info, exc_stack))  # Signal that execution finished with an error.\n",
        "            else:\n",
        "                event_outq.put((\"state:finished\", None, None, None))  # Signal that execution finished successfully.\n",
        "\n",
        "            os.remove(self.agent_file_name)  # Remove the agent file after execution.\n",
        "\n",
        "            result_outq.put(\"<|EOF|>\")  # Put an EOF marker to indicate the end of output.\n",
        "\n",
        "    def create_process(self) -> None:\n",
        "        # Create three queues for communication with the child process:\n",
        "        # - code_inq: for sending code to execute.\n",
        "        # - result_outq: for receiving output from the execution.\n",
        "        # - event_outq: for receiving state events (like ready and finished).\n",
        "        # trunk-ignore(mypy/var-annotated)\n",
        "        self.code_inq, self.result_outq, self.event_outq = Queue(), Queue(), Queue()\n",
        "        self.process = Process(\n",
        "            target=self._run_session,  # Set the target function for the child process.\n",
        "            args=(self.code_inq, self.result_outq, self.event_outq),  # Provide the necessary queues as arguments.\n",
        "        )\n",
        "        self.process.start()  # Start the child process.\n",
        "\n",
        "    def cleanup_session(self):\n",
        "        if self.process is None:  # If there is no process, nothing to clean up.\n",
        "            return\n",
        "        try:\n",
        "            # Attempt to terminate the child process gracefully.\n",
        "            self.process.terminate()  # Request the process to terminate.\n",
        "            self.process.join(timeout=0.5)  # Wait for the process to finish with a 0.5-second timeout.\n",
        "\n",
        "            if self.process.exitcode is None:  # If the process is still running,\n",
        "                self.process.kill()  # Forcefully kill the process.\n",
        "                self.process.join(timeout=0.5)  # Wait again for termination.\n",
        "\n",
        "                if self.process.exitcode is None:  # If the process still hasn't terminated,\n",
        "                    os.kill(self.process.pid, signal.SIGKILL)  # Send a SIGKILL signal.\n",
        "        except Exception as e:\n",
        "            print(f\"Error during process cleanup: {e}\")  # Print an error message if cleanup fails.\n",
        "        finally:\n",
        "            if self.process is not None:  # If the process exists,\n",
        "                self.process.close()  # Close the process.\n",
        "                self.process = None  # Reset the process attribute to None.\n",
        "\n",
        "    def run(self, code: str, reset_session=True) -> ExecutionResult:\n",
        "        \"\"\"\n",
        "        Execute the provided Python command in a separate process and return its output.\n",
        "\n",
        "        Parameters:\n",
        "            code (str): Python code to execute.\n",
        "            reset_session (bool, optional): Whether to reset the interpreter session before executing the code. Defaults to True.\n",
        "\n",
        "        Returns:\n",
        "            ExecutionResult: Object containing the output and metadata of the code execution.\n",
        "        \"\"\"\n",
        "\n",
        "        if reset_session:\n",
        "            if self.process is not None:\n",
        "                # If a previous process exists, clean it up before starting a new one.\n",
        "                self.cleanup_session()\n",
        "            self.create_process()  # Create a new child process.\n",
        "        else:\n",
        "            # For the first execution, reset_session must be True.\n",
        "            assert self.process is not None\n",
        "\n",
        "        assert self.process.is_alive()  # Ensure that the child process is running.\n",
        "\n",
        "        self.code_inq.put(code)  # Send the code to the child process via the queue.\n",
        "\n",
        "        # Wait for the child process to signal that it is ready.\n",
        "        try:\n",
        "            state = self.event_outq.get(timeout=10)  # Wait up to 10 seconds for the \"state:ready\" event.\n",
        "        except queue.Empty:\n",
        "            msg = \"REPL child process failed to start execution\"\n",
        "            print.critical(msg)  # Log a critical error if the process does not start.\n",
        "            while not self.result_outq.empty():\n",
        "                continue  # Drain the result queue.\n",
        "            raise RuntimeError(msg) from None\n",
        "        assert state[0] == \"state:ready\", state  # Verify that the received state is \"state:ready\".\n",
        "        start_time = time.time()  # Record the start time of execution.\n",
        "\n",
        "        child_in_overtime = False  # Flag to indicate if the child process has exceeded the timeout.\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                # Try to get the finished state from the child process.\n",
        "                state = self.event_outq.get(timeout=1)  # Wait for the \"state:finished\" event.\n",
        "                assert state[0] == \"state:finished\", state  # Ensure the state is \"state:finished\".\n",
        "                exec_time = time.time() - start_time  # Calculate the total execution time.\n",
        "                break  # Exit the loop if execution is finished.\n",
        "            except queue.Empty:\n",
        "                # If no event is received, check whether the process is still alive.\n",
        "                if not child_in_overtime and not self.process.is_alive():\n",
        "                    msg = \"REPL child process died unexpectedly\"\n",
        "                    raise RuntimeError(msg) from None\n",
        "\n",
        "                # If the process is still running, check if it has exceeded the timeout.\n",
        "                if self.timeout is None:\n",
        "                    continue\n",
        "                running_time = time.time() - start_time  # Determine the running time.\n",
        "                if running_time > self.timeout:\n",
        "                    print(f\"Execution exceeded timeout of {self.timeout}s\")  # Log a timeout message.\n",
        "                    os.kill(self.process.pid, signal.SIGINT)  # Send SIGINT to the process.\n",
        "                    child_in_overtime = True  # Mark that the process is now in overtime.\n",
        "\n",
        "                    # If the process exceeds the timeout by more than 5 seconds, force cleanup.\n",
        "                    if running_time > self.timeout + 5:\n",
        "                        self.cleanup_session()  # Clean up the child process.\n",
        "\n",
        "                        state = (None, \"TimeoutError\", {}, [])  # Set state to indicate a timeout error.\n",
        "                        exec_time = self.timeout  # Set the execution time to the timeout limit.\n",
        "                        break\n",
        "\n",
        "        output: list[str] = []  # Initialize a list to collect output lines.\n",
        "        # Collect all output from the result queue until the EOF marker is encountered.\n",
        "        start_collect = time.time()  # Record the start time for output collection.\n",
        "        while not self.result_outq.empty() or not output or output[-1] != \"<|EOF|>\":\n",
        "            try:\n",
        "                # If output collection exceeds 5 seconds, log a warning.\n",
        "                if time.time() - start_collect > 5:\n",
        "                    print.warning(\"Output collection timed out\")\n",
        "                    break\n",
        "                output.append(self.result_outq.get(timeout=1))  # Append the next line of output.\n",
        "            except queue.Empty:\n",
        "                continue  # Continue if no output is available immediately.\n",
        "        output.pop()  # Remove the EOF marker from the output list.\n",
        "\n",
        "        # Extract exception information from the finished state.\n",
        "        e_cls_name, exc_info, exc_stack = state[1:]\n",
        "\n",
        "        if e_cls_name == \"TimeoutError\":\n",
        "            # Append a timeout error message to the output if a timeout occurred.\n",
        "            output.append(\n",
        "                f\"TimeoutError: Execution exceeded the time limit of {humanize.naturaldelta(self.timeout)}\"\n",
        "            )\n",
        "        else:\n",
        "            # Append the execution time information to the output.\n",
        "            output.append(\n",
        "                f\"Execution time: {humanize.naturaldelta(exec_time)} seconds (time limit is {humanize.naturaldelta(self.timeout)}).\"\n",
        "            )\n",
        "        # Return an ExecutionResult object with all the execution details.\n",
        "        return ExecutionResult(output, exec_time, e_cls_name, exc_info, exc_stack)\n",
        "\n"
      ],
      "metadata": {
        "id": "e6T1m16_7MCw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Nodes\n"
      ],
      "metadata": {
        "id": "SJy6O6WpnQCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import uuid\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Literal, Optional\n",
        "\n",
        "from dataclasses_json import DataClassJsonMixin\n",
        "\n",
        "\n",
        "@dataclass(eq=False)\n",
        "class Node(DataClassJsonMixin):\n",
        "    \"\"\"A single node in the solution tree. Contains code, execution results, and evaluation information.\"\"\"\n",
        "\n",
        "    # ---- code & plan ----\n",
        "    code: str\n",
        "    plan: str = field(default=None, kw_only=True)  # type: ignore\n",
        "\n",
        "    # ---- general attrs ----\n",
        "    step: int = field(default=None, kw_only=True)  # type: ignore\n",
        "    id: str = field(default_factory=lambda: uuid.uuid4().hex, kw_only=True)\n",
        "    ctime: float = field(default_factory=lambda: time.time(), kw_only=True)\n",
        "    parent: Optional[\"Node\"] = field(default=None, kw_only=True)\n",
        "    children: set[\"Node\"] = field(default_factory=set, kw_only=True)\n",
        "\n",
        "    # ---- execution info ----\n",
        "    _term_out: list[str] = field(default=None, kw_only=True)  # type: ignore\n",
        "    exec_time: float = field(default=None, kw_only=True)  # type: ignore\n",
        "    exc_type: str | None = field(default=None, kw_only=True)\n",
        "    exc_info: dict | None = field(default=None, kw_only=True)\n",
        "    exc_stack: list[tuple] | None = field(default=None, kw_only=True)\n",
        "\n",
        "    # ---- evaluation ----\n",
        "    # post-execution result analysis (findings/feedback)\n",
        "    analysis: str = field(default=None, kw_only=True)  # type: ignore\n",
        "    metric: float = field(default=None, kw_only=True)  # type: ignore\n",
        "    # whether the agent decided that the code is buggy\n",
        "    # -> always True if exc_type is not None or no valid metric\n",
        "    is_buggy: bool = field(default=None, kw_only=True)  # type: ignore\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "        if self.parent is not None:\n",
        "            self.parent.children.add(self)\n",
        "\n",
        "    @property\n",
        "    def stage_name(self) -> Literal[\"draft\", \"debug\", \"improve\"]:\n",
        "        \"\"\"\n",
        "        Return the stage of the node:\n",
        "        - \"stage\" if the node is an initial solution draft\n",
        "        - \"debug\" if the node is the result of a debugging step\n",
        "        - \"improve\" if the node is the result of an improvement step\n",
        "        \"\"\"\n",
        "        if self.parent is None:\n",
        "            return \"draft\"\n",
        "        return \"debug\" if self.parent.is_buggy else \"improve\"\n",
        "\n",
        "    def absorb_exec_result(self, exec_result: ExecutionResult):\n",
        "        \"\"\"Absorb the result of executing the code from this node.\"\"\"\n",
        "        self._term_out = exec_result.term_out\n",
        "        self.exec_time = exec_result.exec_time\n",
        "        self.exc_type = exec_result.exc_type\n",
        "        self.exc_info = exec_result.exc_info\n",
        "        self.exc_stack = exec_result.exc_stack\n",
        "\n",
        "    @property\n",
        "    def term_out(self) -> str:\n",
        "        \"\"\"Get the terminal output of the code execution (after truncating it).\"\"\"\n",
        "        return trim_long_string(\"\".join(self._term_out))\n",
        "\n",
        "    @property\n",
        "    def is_leaf(self) -> bool:\n",
        "        \"\"\"Check if the node is a leaf node in the solution tree.\"\"\"\n",
        "        return not self.children\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return isinstance(other, Node) and self.id == other.id\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash(self.id)\n",
        "\n",
        "    @property\n",
        "    def debug_depth(self) -> int:\n",
        "        \"\"\"\n",
        "        Length of the current debug path\n",
        "        - 0 if the node is not a debug node (parent is not buggy)\n",
        "        - 1 if the parent is buggy but the skip parent isn't\n",
        "        - n if there were n consecutive debugging steps\n",
        "        \"\"\"\n",
        "        if self.stage_name != \"debug\":\n",
        "            return 0\n",
        "        return self.parent.debug_depth + 1  # type: ignore"
      ],
      "metadata": {
        "id": "Ef9JvWJr7Xvg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tree"
      ],
      "metadata": {
        "id": "Xr5hZPyKnO8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Journal(DataClassJsonMixin):\n",
        "    \"\"\"A collection of nodes representing the solution tree.\n",
        "    作為memory之用，讓他從經驗中學習，這個類的作用是把最新版本node加入tree，get best node，以generate summary\"\"\"\n",
        "\n",
        "    nodes: list[Node] = field(default_factory=list)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Node:\n",
        "        return self.nodes[idx]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Return the number of nodes in the journal.\"\"\"\n",
        "        return len(self.nodes)\n",
        "\n",
        "    def append(self, node: Node) -> None:\n",
        "        \"\"\"Append a new node to the journal.\"\"\"\n",
        "        node.step = len(self.nodes)\n",
        "        self.nodes.append(node)\n",
        "\n",
        "    @property\n",
        "    def draft_nodes(self) -> list[Node]:\n",
        "        \"\"\"Return a list of nodes representing intial coding drafts\"\"\"\n",
        "        return [n for n in self.nodes if n.parent is None]\n",
        "\n",
        "    @property\n",
        "    def buggy_nodes(self) -> list[Node]:\n",
        "        \"\"\"Return a list of nodes that are considered buggy by the agent.\"\"\"\n",
        "        return [n for n in self.nodes if n.is_buggy]\n",
        "\n",
        "    @property\n",
        "    def good_nodes(self) -> list[Node]:\n",
        "        \"\"\"Return a list of nodes that are not considered buggy by the agent.\"\"\"\n",
        "        return [n for n in self.nodes if not n.is_buggy]\n",
        "\n",
        "    def get_metric_history(self) -> list[float]:\n",
        "        \"\"\"Return a list of all metric values in the journal.\"\"\"\n",
        "        return [n.metric for n in self.nodes]\n",
        "\n",
        "    def get_good_nodes(self) -> Node:\n",
        "        return [n for n in self.nodes if not n.is_buggy]\n",
        "\n",
        "    def get_best_node(self, only_good=True) -> None | Node:\n",
        "        \"\"\"Return the best solution found so far (node with the highest validation metric).\"\"\"\n",
        "        if only_good:\n",
        "            nodes = self.good_nodes\n",
        "            if not nodes:\n",
        "                return None\n",
        "        else:\n",
        "            nodes = self.nodes\n",
        "        return min(nodes, key=lambda n: n.metric)\n",
        "\n",
        "    def generate_summary(self, include_code: bool = False) -> str:\n",
        "        \"\"\"Generate a summary of the journal for the agent.\"\"\"\n",
        "        summary = []\n",
        "        for n in self.good_nodes:\n",
        "            summary_part = f\"Design: {n.plan}\\n\"\n",
        "            if include_code:\n",
        "                summary_part += f\"Code: {n.code}\\n\"\n",
        "            summary_part += f\"Results: {n.analysis}\\n\"\n",
        "            summary_part += f\"Validation Metric (Mean Squared Error): {n.metric}\\n\"\n",
        "            summary.append(summary_part)\n",
        "        return \"\\n-------------------------------\\n\".join(summary)"
      ],
      "metadata": {
        "id": "hdIKQ-ZHnQmY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent"
      ],
      "metadata": {
        "id": "kHofZDRkfCBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from typing import Any, Callable, cast\n",
        "\n",
        "import re\n",
        "import sys\n",
        "import json\n",
        "import humanize\n",
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "ExecCallbackType = Callable[[str, bool], ExecutionResult]\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        cfg,\n",
        "        journal: Journal,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.journal = journal\n",
        "        self.data_preview: str | None = None\n",
        "\n",
        "    def search_policy(self) -> Node | None:\n",
        "        \"\"\"Select a node to work on (or None to draft a new node).\"\"\"\n",
        "        search_cfg = self.cfg.agent.search\n",
        "\n",
        "        # initial drafting\n",
        "        if len(self.journal.draft_nodes) < search_cfg.num_drafts:\n",
        "            return None\n",
        "\n",
        "        # debugging\n",
        "        if random.random() < search_cfg.debug_prob:\n",
        "            # nodes that are buggy + leaf nodes + debug depth < max debug depth\n",
        "            debuggable_nodes = [\n",
        "                n\n",
        "                for n in self.journal.buggy_nodes\n",
        "                if n.is_leaf\n",
        "            ]\n",
        "            if debuggable_nodes:\n",
        "                return random.choice(debuggable_nodes)\n",
        "\n",
        "\n",
        "        # back to drafting if no nodes to improve\n",
        "        good_nodes = self.journal.good_nodes\n",
        "        if not good_nodes:\n",
        "            return None\n",
        "\n",
        "        # greedy\n",
        "        greedy_node = self.journal.get_best_node()\n",
        "\n",
        "        return greedy_node\n",
        "\n",
        "\n",
        "    def plan_and_code_query(self, system_message, user_message, retries=3) -> tuple[str, str]:\n",
        "        \"\"\"Generate a natural language plan + code in the same LLM call and split them apart.\"\"\"\n",
        "        completion_text = None\n",
        "        for _ in range(retries):\n",
        "\n",
        "            response = generate_response(\n",
        "                myModel,\n",
        "                _messages=[\n",
        "                    {'role': 'system', \"content\": system_message},\n",
        "                    {'role': 'user', \"content\": user_message}\n",
        "                ]\n",
        "            )\n",
        "            completion_text = response\n",
        "            code = extract_code(completion_text)\n",
        "            nl_text = extract_text_up_to_code(completion_text)\n",
        "\n",
        "            if code:\n",
        "                return nl_text, code\n",
        "\n",
        "            print(\"Plan + code extraction failed, retrying...\")\n",
        "        print(\"Final plan + code extraction attempt failed, giving up...\")\n",
        "        return \"\", completion_text\n",
        "    #HW(1/4):下Propmt\n",
        "    def _draft(self) -> Node:\n",
        "\n",
        "        # ================ TODO: ask LLM agents to come up with a solution and then implement ================\n",
        "\n",
        "        system_prompt = \"You are an AI agent.\"\n",
        "\n",
        "        user_prompt = [\n",
        "            \"You have to come up with a solution for machine learning task and then implement this solution in Python.\"\n",
        "            f\"The task is to {str(self.cfg.task_goal)} \",\n",
        "            f'All the provided input data is stored in \"{self.cfg.data_dir}\" directory.',\n",
        "            f\"{str(self.data_preview)}\",\n",
        "            'You have to save the predictions result on testing set in \"/content/submission.csv\".',\n",
        "            'Note that the testing file DOES NOT have the target column.'\n",
        "        ]\n",
        "\n",
        "        system_message = system_prompt\n",
        "        user_message = \"\\n\".join(user_prompt)\n",
        "        plan, code = self.plan_and_code_query(system_message=system_message, user_message=user_message)\n",
        "        return Node(plan=plan, code=code)\n",
        "    # Hw(4/4-2)嘗試改善程式碼\n",
        "    def _improve(self, parent_node: Node) -> Node:\n",
        "\n",
        "        # ================  TODO: ask LLM agent to improve drafts ================\n",
        "\n",
        "        system_prompt = \"You are an AI assistant.\"\n",
        "\n",
        "        user_prompt = [\n",
        "            f\"Task description: {str(self.cfg.task_goal)} \"\n",
        "            f\"Memory: {str(self.journal.generate_summary())} \"\n",
        "            f\"Previous solution: Code: {str(wrap_code(parent_node.code))} \"\n",
        "        ]\n",
        "        system_message = system_prompt\n",
        "        user_message = \" \".join(user_prompt)\n",
        "        plan, code = self.plan_and_code_query(system_message=system_message, user_message=user_message)\n",
        "        return Node(plan=plan, code=code, parent=parent_node)\n",
        "    # Hw(4/4-3)嘗試改善程式碼\n",
        "    def _debug(self, parent_node: Node) -> Node:\n",
        "\n",
        "        # ================  TODO: ask LLM agent to debug ================\n",
        "        system_prompt = \"You are an AI agent.\"\n",
        "\n",
        "\n",
        "        user_prompt = [\n",
        "            f\"Task description: {str(self.cfg.task_goal)}\\n\\n\",\n",
        "            f\"Previous (buggy) implementation: {str(wrap_code(parent_node.code))}\\n\\n\",\n",
        "            f\"Execution output: {str(wrap_code(parent_node.term_out, lang=''))}\\n\\n\",\n",
        "            str(self.data_preview)\n",
        "        ]\n",
        "\n",
        "        system_message = system_prompt\n",
        "        user_message = \" \".join(user_prompt)\n",
        "\n",
        "        plan, code = self.plan_and_code_query(system_message=system_message, user_message=user_message)\n",
        "        return Node(plan=plan, code=code, parent=parent_node)\n",
        "\n",
        "    def update_data_preview(\n",
        "        self,\n",
        "    ):\n",
        "        self.data_preview = data_preview_generate(cfg.data_dir)\n",
        "\n",
        "    def step(self, exec_callback: ExecCallbackType):\n",
        "        if not self.journal.nodes or self.data_preview is None:\n",
        "            self.update_data_preview()\n",
        "\n",
        "        parent_node = self.search_policy()\n",
        "\n",
        "        if parent_node is None:\n",
        "            result_node = self._draft()\n",
        "        elif parent_node.is_buggy:\n",
        "            result_node = self._debug(parent_node)\n",
        "        else:\n",
        "            result_node = self._improve(parent_node)\n",
        "\n",
        "        self.parse_exec_result(\n",
        "            node=result_node,\n",
        "            exec_result=exec_callback(result_node.code, True),\n",
        "        )\n",
        "        self.journal.append(result_node)\n",
        "    #hw(5)幫助選擇要繳交的程式碼\n",
        "    def parse_exec_result(self, node: Node, exec_result: ExecutionResult):\n",
        "        node.absorb_exec_result(exec_result)\n",
        "\n",
        "        system_prompt = \"You are an AI assistant.\"\n",
        "\n",
        "        # ================  TODO: ask LLM agent to extract evaluation result from the execution output. ================\n",
        "        # save log file\n",
        "        user_prompt = f\"\"\"\n",
        "            The task is:\n",
        "            {self.cfg.task_goal}\n",
        "\n",
        "            The code implementation is:\n",
        "            {wrap_code(node.code)}\n",
        "\n",
        "            The execution output is:\n",
        "            {wrap_code(node.term_out, lang=\"\")}\n",
        "        \"\"\"\n",
        "\n",
        "        system_message = system_prompt\n",
        "        user_message = \" \".join(user_prompt)\n",
        "\n",
        "        response = generate_response(\n",
        "            myModel,\n",
        "            _messages=[\n",
        "                {'role': 'system', \"content\": system_message},\n",
        "                {'role': 'user', \"content\": user_message}\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # ================  TODO: evaluation ================\n",
        "        # you can force the LLM to structure the output to extract the metric\n",
        "        # 以下連結可以學習，如何讓大型語言模型兔出我們要的output，當然也可以用正則表示法清理\n",
        "        # reference: https://python.useinstructor.com/integrations/llama-cpp-python/#llama-cpp-python\n",
        "        #這是一個使用 llama-cpp-python 和 Instructor 生成結構化輸出的完整指南。您將學到如何使用 JSON schema 模式和推測解碼來從本地 LLMs 生成類型安全的回應。\n",
        "        # node.analysis = response.summary\n",
        "        # node.is_buggy = (\n",
        "        #     response.is_buggy\n",
        "        #     or node.exc_type is not None\n",
        "        #     or response.metric is None\n",
        "        # )\n",
        "\n",
        "        node.is_buggy = False\n",
        "        node.metric = 0.0\n"
      ],
      "metadata": {
        "id": "5Sg4cBTy7JIO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Processing"
      ],
      "metadata": {
        "id": "di-9OGwOiZgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def wrap_code(code: str, lang=\"python\") -> str:\n",
        "    \"\"\"Wraps code with three backticks.\"\"\"\n",
        "    return f\"```{lang}\\n{code}\\n```\"\n",
        "\n",
        "\n",
        "def is_valid_python_script(script):\n",
        "    \"\"\"Check if a script is a valid Python script.\"\"\"\n",
        "    try:\n",
        "        compile(script, \"<string>\", \"exec\")\n",
        "        return True\n",
        "    except SyntaxError:\n",
        "        return False\n",
        "\n",
        "\n",
        "def extract_jsons(text):\n",
        "    \"\"\"Extract all JSON objects from the text. Caveat: This function cannot handle nested JSON objects.\"\"\"\n",
        "    json_objects = []\n",
        "\n",
        "    # Find {} by regular expression\n",
        "    matches = re.findall(r\"\\{.*?\\}\", text, re.DOTALL)\n",
        "\n",
        "    # Try to transform string into json objects\n",
        "    for match in matches:\n",
        "        try:\n",
        "            json_obj = json.loads(match)\n",
        "            json_objects.append(json_obj)\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "\n",
        "    return json_objects\n",
        "\n",
        "def trim_long_string(string, threshold=5100, k=2500):\n",
        "    # Check if the length of the string is longer than the threshold\n",
        "    if len(string) > threshold:\n",
        "        # Output the first k and last k characters\n",
        "        first_k_chars = string[:k]\n",
        "        last_k_chars = string[-k:]\n",
        "\n",
        "        truncated_len = len(string) - 2 * k\n",
        "\n",
        "        return f\"{first_k_chars}\\n ... [{truncated_len} characters truncated] ... \\n{last_k_chars}\"\n",
        "    else:\n",
        "        return string\n",
        "\n",
        "def extract_code(text):\n",
        "    \"\"\"Extract python code blocks from the text.\"\"\"\n",
        "    parsed_codes = []\n",
        "\n",
        "    # When code is in a text or python block\n",
        "    matches = re.findall(r\"```(python)?\\n*(.*?)\\n*```\", text, re.DOTALL)\n",
        "    for match in matches:\n",
        "        code_block = match[1]\n",
        "        parsed_codes.append(code_block)\n",
        "\n",
        "    # When the entire text is code or backticks of the code block is missing\n",
        "    if len(parsed_codes) == 0:\n",
        "        matches = re.findall(r\"^(```(python)?)?\\n?(.*?)\\n?(```)?$\", text, re.DOTALL)\n",
        "        if matches:\n",
        "            code_block = matches[0][2]\n",
        "            parsed_codes.append(code_block)\n",
        "\n",
        "    # validate the parsed codes\n",
        "    valid_code_blocks = [\n",
        "        c for c in parsed_codes if is_valid_python_script(c)\n",
        "    ]\n",
        "    return \"\\n\\n\".join(valid_code_blocks)\n",
        "\n",
        "def extract_text_up_to_code(s):\n",
        "    \"\"\"Extract (presumed) natural language text up to the start of the first code block.\"\"\"\n",
        "    if \"```\" not in s:\n",
        "        return \"\"\n",
        "    return s[: s.find(\"```\")].strip()\n",
        "\n"
      ],
      "metadata": {
        "id": "S1csIXAO6i8i"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Selection"
      ],
      "metadata": {
        "id": "26WSVJyCnC1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import humanize\n",
        "import pandas as pd\n",
        "\n",
        "# HW(2/4) 選擇適合的feature(可以自選也可以讓模型判斷，現在res是直接列出feature，沒有加註說明)\n",
        "def preview_csv(p: Path) -> str:\n",
        "    \"\"\"Generate a textual preview of a csv file\"\"\"\n",
        "\n",
        "    df = pd.read_csv(p)\n",
        "\n",
        "    out = []\n",
        "\n",
        "    out.append(f\"-> {str(p)} has {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
        "\n",
        "    # ================  TODO: Tell LLM agents which feature is useful for prediction ================\n",
        "\n",
        "    cols = df.columns.tolist()\n",
        "    cols_str = \", \".join(cols)\n",
        "\n",
        "    res =  f\"\"\"The dataset contains the following columns: {cols_str}.Compute the correlation matrix and select features with a strong correlation to the target.\"\"\"\n",
        "    # res =  f\"\"\"The dataset contains the following columns: {cols_str}.For predicting 'tested_positive_day3', we select key features:  跳不出submission\n",
        "    #       - Health: 'cli_day1', 'ili_day1', 'hh_cmnty_cli_day1',\n",
        "    #       - Behavior: 'wearing_mask_7d_day1', 'wothers_masked_public_day1',\n",
        "    #       - Belief: 'wbelief_masking_effective_day1', 'wbelief_distancing_effective_day1',\n",
        "    #       - Trend: 'cli_day2 - cli_day1'.\n",
        "            #Select 'cli_day1', 'ili_day1', 'hh_cmnty_cli_day1', 'wearing_mask_7d_day1', 'wothers_masked_public_day1','wbelief_masking_effective_day1', 'wbelief_distancing_effective_day1' as the most representative features.\n",
        "\n",
        "    #     \"\"\"\n",
        "    # res =  f\"\"\"\n",
        "    #       The dataset contains the following columns: {cols_str}.\n",
        "\n",
        "    #       ### Task:\n",
        "    #       Given survey results from the past two days in a specific U.S. state,\n",
        "    #       predict the probability of testing positive on day 3.\n",
        "\n",
        "    #       ### Evaluation Metric:\n",
        "    #       Mean Squared Error (MSE).\n",
        "\n",
        "    #       ### Feature Selection Guidelines:\n",
        "    #       1. **Define the Target Variable**\n",
        "    #         - The target variable should be 'probability of testing positive on day 3'.\n",
        "    #         - Exclude any features that directly report day 3 results (to prevent data leakage).\n",
        "\n",
        "    #       2. **Identify Feature Types**\n",
        "    #         - Separate numerical and categorical features.\n",
        "    #         - Numerical: Continuous variables like age, symptom severity, exposure count.\n",
        "    #         - Categorical: State, gender, symptom type, previous test results (one-hot or label encoding may be needed).\n",
        "\n",
        "    #       3. **Feature Selection Process**\n",
        "    #         - Compute the correlation matrix and select features with a strong correlation to the target.\n",
        "    #         - Use Mutual Information (MI) to assess categorical features.\n",
        "    #         - Drop highly correlated independent variables to prevent multicollinearity.\n",
        "    #         - Consider using feature importance scores from models like Random Forest or SHAP values.\n",
        "\n",
        "    #       4. **Time-Series Consideration**\n",
        "    #         - Prioritize features from the past two days that may influence the outcome on day 3.\n",
        "    #         - Look at trends in symptoms, exposure levels, and regional infection rates.\n",
        "\n",
        "    #       \"\"\"\n",
        "    out.append(res)\n",
        "\n",
        "    return \"\\n\".join(out)\n",
        "\n",
        "def data_preview_generate(base_path):\n",
        "    \"\"\"\n",
        "    Generate a textual preview of a directory\n",
        "    \"\"\"\n",
        "\n",
        "    result = []\n",
        "    files = [p for p in Path(base_path).iterdir()]\n",
        "    for f in sorted(files):\n",
        "        result.append(preview_csv(f))\n",
        "\n",
        "    result = \"\\n\\n\".join(result)\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "f8hRG2o7yeoc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Config"
      ],
      "metadata": {
        "id": "8qeTrqDrbVtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# DO NOT MODIFY THIS CELL\n",
        "class Config:\n",
        "    \"\"\"\n",
        "    A recursive configuration class that converts a dictionary into an object\n",
        "    with attributes accessible using dot notation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dictionary):\n",
        "        for key, value in dictionary.items():\n",
        "            if isinstance(value, dict):\n",
        "                value = Config(value)\n",
        "            setattr(self, key, value)\n",
        "\n",
        "def set_seed(seed=531):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_seed()"
      ],
      "metadata": {
        "id": "-yeyuK6n6tY5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HW(4/4-1):嘗試讓他迭代多次，要用的話要同時調整temperature\n",
        "# ================  TODO: config ================\n",
        "config = {\n",
        "    # experiment configurations\n",
        "    \"exp_name\": \"ML2025_HW2\",\n",
        "    \"data_dir\":  Path(\"/content/ML2025Spring-hw2-public\").resolve(),\n",
        "\n",
        "    # the description of the task\n",
        "    \"task_goal\": \"Given the survey results from the past two days in a specific state in the U.S.,\\\n",
        "                  predict the probability of testing positive on day 3. \\\n",
        "                  The evaluation metric is Mean Squared Error (MSE).\",\n",
        "\n",
        "    \"agent\": {\n",
        "        # the number of iterations\n",
        "        #１次表示完全不會做iteration，這裡可以調整多次，讓他　draft　more\n",
        "        \"steps\": 2,\n",
        "        \"search\": {\n",
        "            # decide whether to debug or improve\n",
        "            \"debug_prob\": 0.5,\n",
        "            #希望他從頭開始寫幾次\n",
        "            # the number of draft generated before improving/debugging\n",
        "            # \"num_drafts\": 1,\n",
        "            \"num_drafts\": 3,\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "cfg = Config(config)"
      ],
      "metadata": {
        "id": "6rGMZ96i6LwT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "z_DTmIU8_Pkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    def exec_callback(*args, **kwargs):\n",
        "        res = interpreter.run(*args, **kwargs)\n",
        "        return res\n",
        "\n",
        "    journal = Journal()\n",
        "    agent = Agent(\n",
        "        cfg=cfg,\n",
        "        journal=journal,\n",
        "    )\n",
        "\n",
        "    interpreter = Interpreter()\n",
        "\n",
        "    global_step = len(journal)\n",
        "    while global_step < cfg.agent.steps:\n",
        "        # run agent\n",
        "        agent.step(exec_callback=exec_callback)\n",
        "        # save results for this iteration\n",
        "        save_run(cfg, journal)\n",
        "        # get currect step\n",
        "        global_step = len(journal)\n",
        "\n",
        "\n",
        "    # Kill created child process\n",
        "    interpreter.cleanup_session()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "YHG21H719_A3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get your best result!\n",
        "# !python best_solution.py"
      ],
      "metadata": {
        "id": "uD5bPjF5dO2Y"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "The code scripts are from [aideml](https://github.com/WecoAI/aideml) project on github with some modifications.\n",
        "\n",
        "AIDE: AI-Driven Exploration in the Space of Code\n",
        "https://arxiv.org/pdf/2502.13138\n"
      ],
      "metadata": {
        "id": "feFaoy8tbvyC"
      }
    }
  ]
}